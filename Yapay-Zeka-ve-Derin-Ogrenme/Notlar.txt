MNIST Dataseti
* El yazısı rakamlardan oluşur
* 28x28 pixel
* Toplam 70.000 resim mevcut (60.000 eğitim - 10.000 test)


Layer (Katman) Türleri
* Fully connected (dense) layers
* Convolutional layers
* Pooling layers
* Recuurent layers
* Normalizastion layers
* ...


Aktivasyon Fonksiyonları
1. Sigmoid
	* Sayıları [0, 1] arasına sıkıştırır
	* Geçmişte oldukça popülerdi
	- Gradient ölümü olabiliyor
	- 0 odaklı değil
	- exp() hesaplanması yavaş
2. tanh
	* [-1, 1] arasına sıkıştırır
	+ Sıfır odaklı
	- Hala gradient ölümü olabiliyor
3. ReLU
	* Negatifse 0, pozitifse olduğu gibi geçiyor
	+ Gradient ölümü yok
	+ Bilgisayarın hesaplaması daha kolay
	+ Biyolojik nörona daha yakın
	- 0 odaklı değil
	- Bazı nöronlar ölebiliyor
4. Leaky ReLU
	+ ReLU'nun getirdiği tüm avantajlar
	+ Sıfır odaklı olduğu için nöron ölümü yok
5. ELU - Exponential Linear Unit
	+ ReLU'nun getirdiği tüm avantajlar
	- exp() kullandığı için biraz daha yavaş
6. Maxout
	* ReLU ve Leaky ReLU'yu genelleştiriyor
	* Nöron ölümü yok
	- Parametre sayısını iki katına çıkarıyor

* İlk olarak ReLU kullanın.
* Leaky ReLU, mevcut, ELU deneyebilirsiniz. Ancak bu fonksiyonlar henüz olgunlaşmamış, deneme aşamasındalar.
* tanh kullanılabilir ama ReLU çoğu durumda daha iyi olacaktır.
* Sigmoid kullanmayın.


Loss (Cost) Fonksiyonları
* Tahmin edilen değerin gerçek değerden ne kadar uzak olduğunu hesaplar.
* Eğitim esnasında zamanla sıfıra yaklaşmasını bekleriz.
* L2 Loss Fonksiyonu en çok kullanılan fonksiyondur.
* Cross Entropy Loss Fonksiyonu


Öğrenme Oranı (Learning Rate)
* Atacağımız adımı sembolize eder
* Çok küçük bir sayı atanır (Örn. 0.0005)
* Büyük olursa minimuma ulaşılamaz
* Küçük olursa model yavaş öğrenir


BackPropagation
* Backpropagation yapay sinir ağlarında her nöronun hataya ne kadar katkısı olduğunu hesaplamak için kullanılır.
* Weight ve bias değerlerini ayarlar.


Overfitting
* Eğitilen modelin eğitim setini çok iyi öğrenip genelleştirme yapamamasına overfitting denir.
* Underfitting ise tam tersi, modelin veriyi hiç öğrenememesi.


Regularization (Düzenlileştirme)
Regularization modelde kompleksliği cezalandırarak daha iyi genelleştirme yapmasını sağlar.
Regularization yöntemleri şu şekilde:
* Daha fazla data ekle
* Data Augmentation
* Loss fonksiyonuna ekstra fonksiyon ekle
* Dropout


Konvolüsyonel Sinir Ağları (CNN - Convolutional Neural Network - ConvNet)
* Filtreler Nasıl Kayıyor?
7x7 input
3x3 filtre
5x5 output

Filtreler farklı sayıda kaydırılabilir, buna stride deniliyor.
7x7 input
3x3 filtre
stride = 2
3x3 output

stride = 3 yapılırsa sığmayacağından uygulanamıyor

Hesaplama
((N - F) / stride) + 1
N: resim boyutu
F: filtre boyutu
stride: kaydırma sayısı

Boyutun Küçülmesine Çözüm
Kenarlara 0 koyarsak, boyut aynı kalacaktır.

Pooling Layer
* Parametre sayısını azaltmak için max pooling kullanıyoruz.
* Her aktivasyon haritası için ayrı ayrı uygulanır. (Derinliği etkilemez)
* Genelde pooling ile boyutlar yarıya indirilir. Bunu yapmanın en yaygın yolu max pooling dir.
	4x4 input
	2x2 filtre
	strides = 2 (çünkü pixellerin çakışmasını istemiyoruz)
	2x2 output
	Burada oluşan output içerisindeki değerler, 2x2 filtre uygularkenki matris içindeki max değerlerdir.




